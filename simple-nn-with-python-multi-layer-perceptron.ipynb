{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# How to build a simple Neural Network with Python: Multi-layer Perceptron\n\n## Table of Contents\n\n1. [Basics of Artificial Neural Networks](#Basics of Artificial Neural Networks)\n\n    1.1 [Single-layer and Multi-layer perceptron](#Single-layer and Multi-layer perceptron)\n    \n    1.2 [About the dataset](#About the dataset)\n    \n    1.3 [The Data](#The Data)\n    \n2. [Perceptron](#Perceptron)\n\n    2.1 [Activation functions](#Activation functions)\n    \n3. [Neural Network's Layer(s)](#Neural Network's Layer(s))\n\n    3.1 [Backpropagation and Gradien Descent](#Backpropagation and Gradien Descent)\n    \n      3.1.1 [TL;DR: (a.k.a: recap](#TL;DR: (a.k.a: recap))\n   \n   3.2 [Our ANN](#Our ANN)\n   \n4. [Training the model](#Training the model)\n   \n   4.1 [Define the Training as a Function](#Define the Training as a Function)\n   \n5. [Compute Predictions](#Compute Predictions)\n   \n   5.1 [Evaluation report](#Evaluation report)\n   \n   5.2 [Exporting the predictions and submit them](#Exporting the predictions and submit them)\n\n6. [The ANN as a class](#The ANN as a class)\n\n    6.1 [An initialization Improvement](#An initialization Improvement)\n    \n    6.2 [Conclusions](#Conclusions)","metadata":{}},{"cell_type":"markdown","source":"# Basics of Artificial Neural Networks <a></a>\n\n\n![ANN](https://upload.wikimedia.org/wikipedia/commons/4/46/Colored_neural_network.svg)\n\n### Single-layer and Multi-layer perceptrons\n\nå¤šå±‚æ„ŸçŸ¥å™¨(Multi-Layer Perceptronï¼ŒMLP)ä¹Ÿå«äººå·¥ç¥ç»ç½‘ç»œ(Artificial Neural Networkï¼ŒANN)ï¼Œé™¤äº†è¾“å…¥è¾“å‡ºå±‚ï¼Œå®ƒä¸­é—´å¯ä»¥æœ‰å¤šä¸ªéšå±‚ã€‚æœ€ç®€å•çš„MLPéœ€è¦æœ‰ä¸€å±‚éšå±‚ï¼Œå³è¾“å…¥å±‚ã€éšå±‚å’Œè¾“å‡ºå±‚æ‰èƒ½ç§°ä¸ºä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œã€‚ä¹ æƒ¯åŸå› æˆ‘ä¹‹åä¼šç§°ä¸ºç¥ç»ç½‘ç»œã€‚é€šä¿—è€Œè¨€ï¼Œç¥ç»ç½‘ç»œæ˜¯ä»¿ç”Ÿç‰©ç¥ç»ç½‘ç»œè€Œæ¥çš„ä¸€ç§æŠ€æœ¯ï¼Œé€šè¿‡è¿æ¥å¤šä¸ªç‰¹å¾å€¼ï¼Œç»è¿‡çº¿æ€§å’Œéçº¿æ€§çš„ç»„åˆï¼Œæœ€ç»ˆè¾¾åˆ°ä¸€ä¸ªç›®æ ‡ï¼Œè¿™ä¸ªç›®æ ‡å¯ä»¥æ˜¯è¯†åˆ«è¿™ä¸ªå›¾ç‰‡æ˜¯ä¸æ˜¯ä¸€åªçŒ«ï¼Œæ˜¯ä¸æ˜¯ä¸€æ¡ç‹—æˆ–è€…å±äºå“ªä¸ªåˆ†å¸ƒã€‚\n\nå•å±‚æ„ŸçŸ¥å™¨(SLP)æ˜¯åŸºäºé˜ˆå€¼ä¼ é€’å‡½æ•°çš„å‰é¦ˆç½‘ç»œã€‚SLPæ˜¯æœ€ç®€å•çš„äººå·¥ç¥ç»ç½‘ç»œç±»å‹ï¼Œåªèƒ½ç”¨äºŒå…ƒç›®æ ‡(1,0)å¯¹çº¿æ€§å¯åˆ†çš„æƒ…å†µè¿›è¡Œåˆ†ç±»ã€‚\n\nå› ä¸ºSLPæ˜¯ä¸€ä¸ªçº¿æ€§åˆ†ç±»å™¨å¦‚æœæƒ…å†µä¸æ˜¯çº¿æ€§å¯åˆ†çš„å­¦ä¹ è¿‡ç¨‹æ°¸è¿œä¸ä¼šè¾¾åˆ°æ‰€æœ‰æƒ…å†µéƒ½è¢«æ­£ç¡®åˆ†ç±»çš„ç‚¹ã€‚æ„ŸçŸ¥å™¨æ— æ³•è§£å†³çº¿æ€§ä¸å¯åˆ†é—®é¢˜çš„æœ€è‘—åçš„ä¾‹å­å°±æ˜¯å¼‚æˆ–é—®é¢˜ã€‚\n\n\nå¤šå±‚æ„ŸçŸ¥å™¨(MLP)å…·æœ‰ä¸å•å±‚æ„ŸçŸ¥å™¨ç›¸åŒçš„ç»“æ„ï¼Œå…·æœ‰ä¸€ä¸ªæˆ–å¤šä¸ªéšè—å±‚ã€‚åå‘ä¼ æ’­ç®—æ³•åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µ:æ¿€æ´»çš„å‰è¿›é˜¶æ®µä¼ æ’­ä»è¾“å…¥åˆ°è¾“å‡ºå±‚ä¹‹é—´çš„è¯¯å·®å’Œè½åçš„é˜¶æ®µ,è§‚å¯Ÿåˆ°çš„å®é™…å’Œè¯·æ±‚çš„æ ‡ç§°å€¼åœ¨è¾“å‡ºå±‚å‘åä¼ æ’­ä»¥ä¿®æ”¹é‡é‡å’Œåå·®å€¼ã€‚\n\nA single layer perceptron (SLP) is a feed-forward network based on a threshold transfer function. SLP is the simplest type of artificial neural networks and can only classify linearly separable cases with a binary target (1, 0). [c], [d]\n\nBecause SLP is a linear classifier and if the cases are not linearly separable the learning process will never reach a point where all the cases are classified properly. The most famous example of the inability of perceptron to solve problems with linearly non-separable cases is the XOR problem.\n\nA multi-layer perceptron (MLP) has the same structure of a single layer perceptron with one or more hidden layers. The backpropagation algorithm consists of two phases: the forward phase where the activations are propagated from the input to the output layer, and the backward phase, where the error between the observed actual and the requested nominal value in the output layer is propagated backwards in order to modify the weights and bias values.\n\n## About the Dataset\n\n### Overview\nThe dataset is the one from the kaggle competion *[Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic/data)* \n\nThe data has been split into two groups:\n    * training set (train.csv)\n    * test set (test.csv)\n\nThe training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the â€œground truthâ€) for each passenger. Your model will be based on â€œfeaturesâ€ like passengersâ€™ gender and class. You can also use feature engineering to create new features.\n\nThe test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\n\nWe also include gender_submission.csv, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.\n\n\n\n-------\n[1] https://en.wikipedia.org/wiki/Perceptron\n\n[2] https://en.wikipedia.org/wiki/Artificial_neural_network\n\n[3] https://www.saedsayad.com/artificial_neural_network_bkp.htm\n\n[4] https://iamtrask.github.io/2015/07/12/basic-python-network/\n\n[5] https://www.freecodecamp.org/news/building-a-neural-network-from-scratch/\n\n[6] https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6?","metadata":{}},{"cell_type":"markdown","source":"ç”±æ­¤å¯çŸ¥ï¼Œç¥ç»ç½‘ç»œä¸»è¦æœ‰ä¸‰ä¸ªåŸºæœ¬è¦ç´ ï¼šæƒé‡ã€åç½®å’Œæ¿€æ´»å‡½æ•°\n\næƒé‡ï¼šç¥ç»å…ƒä¹‹é—´çš„è¿æ¥å¼ºåº¦ç”±æƒé‡è¡¨ç¤ºï¼Œæƒé‡çš„å¤§å°è¡¨ç¤ºå¯èƒ½æ€§çš„å¤§å°\n\nåç½®ï¼šåç½®çš„è®¾ç½®æ˜¯ä¸ºäº†æ­£ç¡®åˆ†ç±»æ ·æœ¬ï¼Œæ˜¯æ¨¡å‹ä¸­ä¸€ä¸ªé‡è¦çš„å‚æ•°ï¼Œå³ä¿è¯é€šè¿‡è¾“å…¥ç®—å‡ºçš„è¾“å‡ºå€¼ä¸èƒ½éšä¾¿æ¿€æ´»ã€‚\n\næ¿€æ´»å‡½æ•°ï¼šèµ·éçº¿æ€§æ˜ å°„çš„ä½œç”¨ï¼Œå…¶å¯å°†ç¥ç»å…ƒçš„è¾“å‡ºå¹…åº¦é™åˆ¶åœ¨ä¸€å®šèŒƒå›´å†…ï¼Œä¸€èˆ¬é™åˆ¶åœ¨ï¼ˆ-1~1ï¼‰æˆ–ï¼ˆ0~1ï¼‰ä¹‹é—´ã€‚æœ€å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°æ˜¯Sigmoidå‡½æ•°ï¼Œå…¶å¯å°†ï¼ˆ-âˆï¼Œ+âˆï¼‰çš„æ•°æ˜ å°„åˆ°ï¼ˆ0~1ï¼‰çš„èŒƒå›´å†…ã€‚","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nnp.random.seed(10)\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Required magic to display matplotlib plots in notebooks\n%matplotlib inline\n\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the \"../input/\" directory.\n        \ndata = pd.read_csv('../input/titanic/train.csv')\n\ndata.head(4)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# å…³äºæœ¬æ•°æ®é›†\n\n   irisåŒ…å«150ä¸ªæ ·æœ¬ï¼Œå¯¹åº”æ•°æ®é›†çš„æ¯è¡Œæ•°æ®ã€‚æ¯è¡Œæ•°æ®åŒ…å«æ¯ä¸ªæ ·æœ¬çš„å››ä¸ªç‰¹å¾å’Œæ ·æœ¬çš„ç±»åˆ«ä¿¡æ¯ï¼Œæ‰€ä»¥irisæ•°æ®é›†æ˜¯ä¸€ä¸ª150è¡Œ5åˆ—çš„äºŒç»´è¡¨ã€‚irisåŒ…å«150ä¸ªæ ·æœ¬ï¼Œå¯¹åº”æ•°æ®é›†çš„æ¯è¡Œæ•°æ®ã€‚æ¯è¡Œæ•°æ®åŒ…å«æ¯ä¸ªæ ·æœ¬çš„å››ä¸ªç‰¹å¾å’Œæ ·æœ¬çš„ç±»åˆ«ä¿¡æ¯ï¼Œæ‰€ä»¥irisæ•°æ®é›†æ˜¯ä¸€ä¸ª150è¡Œ5åˆ—çš„äºŒç»´è¡¨ã€‚\n\n\n![dattaset](http://datahref.com/wp-content/uploads/2016/06/Iris_dataset_scatterplot.svg_.png)\n","metadata":{}},{"cell_type":"markdown","source":"# Perceptron\n\n\næ„ŸçŸ¥å™¨æ˜¯æ¨¡æ‹Ÿäººç±»ç¥ç»å…ƒçš„åŸºæœ¬åŠŸèƒ½ã€‚å®ƒæ¥æ”¶ğ‘›è¾“å…¥ï¼Œä¸æ ‘çªè¾“å…¥çš„ç¥ç»å…ƒç›¸å…³è”ã€‚ç”±äºå€¾æ–œï¼Œæ¯ä¸ªæ ‘çªéƒ½è¢«ä¸€ä¸ªæ•°å­—åŠ æƒï¼Œè¿™ä¸ªæ•°å­—è¡¨ç¤ºå®ƒä¸ç¥ç»å…ƒ[1]çš„è¾“å…¥ç›¸å…³æ€§ã€‚\n\nThe perceptron is a basic function that mimics the human neuron. It receives $n$ inputs, associated to the dendrites inputs to the neuron. Each dendrite, due to *lernging*, is weighted by a number that signals its input relevance for the neuron [1]. \n\n![Neuron](https://upload.wikimedia.org/wikipedia/commons/a/a9/Complete_neuron_cell_diagram_en.svg)\n\n\nä¿¡å·å°±è¿™æ ·è¢«åˆ¶é€ å‡ºæ¥ï¼Œå¹¶é€šè¿‡*è½´çª*ä¼ é€’ç»™å…¶ä»–ç¥ç»å…ƒ[2];å®é™…ä¸Šï¼Œåªæœ‰å½“ç²¾å¿ƒè®¾è®¡çš„è¾“å…¥è¶…è¿‡æŸä¸ªé˜ˆå€¼æ—¶ï¼Œç¥ç»å…ƒæ‰ä¼šå‘å‡ºä¿¡å·;è¿™æ˜¯ä¸€ä¸ªå°–å³°ç¥ç»å…ƒ\nThe signal is thus elaborated and passed through the *axon* to others neurons [2]; actually, the neurons *fires* the signal only if the elaborated inputs have surpassed a certain threshold; this is a spiking neuron [3].\n\n\næ„ŸçŸ¥å™¨æƒ³è¦æ¨¡ä»¿å®ƒã€‚æ¥æ”¶ä¸€ä¸ªå‘é‡(å³æ•°ç»„)$x_i$ä¿¡å·ï¼Œå…¶ä¸­$i$ä»£è¡¨$i$-thæ ‘çªï¼Œå®ƒç”¨ä¸€ä¸ª$w_i$å‘é‡å¯¹æ¯ä¸ªæ ‘çªåŠ æƒã€‚å®ƒè¿˜å¢åŠ äº†ä¸€ä¸ªâ€œåå·®â€æ¥æ¶ˆé™¤æ¥è¿‘é›¶çš„é—®é¢˜(åå·®å°†å†³ç­–è¾¹ç•Œä»åŸç‚¹ç§»å¼€ï¼Œå¹¶ä¸”ä¸ä¾èµ–äºä»»ä½•è¾“å…¥å€¼)ã€‚\n\nThe perceptron wants to mimic it. Receinving a vector (i.e. array) $x_i$ of signals, where $i$ stands for the $i$-th dendrites, it weights each of them by a vector of weights $w_i$. It adds also a *bias* to remove near-zero issues (the bias shifts the decision boundary away from the origin and does not depend on any input value).\n\n![Perceptron](https://miro.medium.com/max/2870/1*n6sJ4yZQzwKL9wnF5wnVNg.png)\n\n### Activation functions\n\næ¿€æ´»å‡½æ•°ï¼ˆactivation functionï¼‰é€šè¿‡è®¡ç®—åŠ æƒå’Œå¹¶åŠ ä¸Šåç½®æ¥ç¡®å®šç¥ç»å…ƒæ˜¯å¦åº”è¯¥è¢«æ¿€æ´»ï¼Œ å®ƒä»¬å°†è¾“å…¥ä¿¡å·è½¬æ¢ä¸ºè¾“å‡ºçš„å¯å¾®è¿ç®—ã€‚ å¤§å¤šæ•°æ¿€æ´»å‡½æ•°éƒ½æ˜¯éçº¿æ€§çš„ã€‚\n\n####  ReLUå‡½æ•°\næœ€å—æ¬¢è¿çš„æ¿€æ´»å‡½æ•°æ˜¯ä¿®æ­£çº¿æ€§å•å…ƒï¼ˆRectified linear unitï¼ŒReLUï¼‰ï¼Œ å› ä¸ºå®ƒå®ç°ç®€å•ï¼ŒåŒæ—¶åœ¨å„ç§é¢„æµ‹ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ã€‚ ReLUæä¾›äº†ä¸€ç§éå¸¸ç®€å•çš„éçº¿æ€§å˜æ¢ã€‚ ç»™å®šå…ƒç´ ï¼ŒReLUå‡½æ•°è¢«å®šä¹‰ä¸ºè¯¥å…ƒç´ ä¸çš„æœ€å¤§å€¼ï¼š\n$$\\operatorname{ReLU}(x) = \\max(x, 0).$$\n\n\né€šä¿—åœ°è¯´ï¼ŒReLUå‡½æ•°é€šè¿‡å°†ç›¸åº”çš„æ´»æ€§å€¼è®¾ä¸º0ï¼Œä»…ä¿ç•™æ­£å…ƒç´ å¹¶ä¸¢å¼ƒæ‰€æœ‰è´Ÿå…ƒç´ ã€‚ ä¸ºäº†ç›´è§‚æ„Ÿå—ä¸€ä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥ç”»å‡ºå‡½æ•°çš„æ›²çº¿å›¾ã€‚ æ­£å¦‚ä»å›¾ä¸­æ‰€çœ‹åˆ°ï¼Œæ¿€æ´»å‡½æ•°æ˜¯åˆ†æ®µçº¿æ€§çš„ã€‚\n\n![5555](http://zh.d2l.ai/_images/output_mlp_76f463_21_0.svg)\n\n\nå½“è¾“å…¥ä¸ºè´Ÿæ—¶ï¼ŒReLUå‡½æ•°çš„å¯¼æ•°ä¸º0ï¼Œè€Œå½“è¾“å…¥ä¸ºæ­£æ—¶ï¼ŒReLUå‡½æ•°çš„å¯¼æ•°ä¸º1ã€‚ æ³¨æ„ï¼Œå½“è¾“å…¥å€¼ç²¾ç¡®ç­‰äº0æ—¶ï¼ŒReLUå‡½æ•°ä¸å¯å¯¼ã€‚ åœ¨æ­¤æ—¶ï¼Œæˆ‘ä»¬é»˜è®¤ä½¿ç”¨å·¦ä¾§çš„å¯¼æ•°ï¼Œå³å½“è¾“å…¥ä¸º0æ—¶å¯¼æ•°ä¸º0ã€‚ æˆ‘ä»¬å¯ä»¥å¿½ç•¥è¿™ç§æƒ…å†µï¼Œå› ä¸ºè¾“å…¥å¯èƒ½æ°¸è¿œéƒ½ä¸ä¼šæ˜¯0ã€‚ è¿™é‡Œå¼•ç”¨ä¸€å¥å¤è€çš„è°šè¯­ï¼Œâ€œå¦‚æœå¾®å¦™çš„è¾¹ç•Œæ¡ä»¶å¾ˆé‡è¦ï¼Œæˆ‘ä»¬å¾ˆå¯èƒ½æ˜¯åœ¨ç ”ç©¶æ•°å­¦è€Œéå·¥ç¨‹â€ï¼Œ è¿™ä¸ªè§‚ç‚¹æ­£å¥½é€‚ç”¨äºè¿™é‡Œã€‚ ä¸‹é¢æˆ‘ä»¬ç»˜åˆ¶ReLUå‡½æ•°çš„å¯¼æ•°ã€‚\n\n\n![666](http://zh.d2l.ai/_images/output_mlp_76f463_33_0.svg)\n\nä½¿ç”¨ReLUçš„åŸå› æ˜¯ï¼Œå®ƒæ±‚å¯¼è¡¨ç°å¾—ç‰¹åˆ«å¥½ï¼šè¦ä¹ˆè®©å‚æ•°æ¶ˆå¤±ï¼Œè¦ä¹ˆè®©å‚æ•°é€šè¿‡ã€‚ è¿™ä½¿å¾—ä¼˜åŒ–è¡¨ç°å¾—æ›´å¥½ï¼Œå¹¶ä¸”ReLUå‡è½»äº†å›°æ‰°ä»¥å¾€ç¥ç»ç½‘ç»œçš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚\n\n#### sigmoidå‡½æ•°\n\nå¯¹äºä¸€ä¸ªå®šä¹‰åŸŸåœ¨ä¸­çš„è¾“å…¥ï¼Œ sigmoidå‡½æ•°å°†è¾“å…¥å˜æ¢ä¸ºåŒºé—´(0, 1)ä¸Šçš„è¾“å‡ºã€‚ å› æ­¤ï¼Œsigmoidé€šå¸¸ç§°ä¸ºæŒ¤å‹å‡½æ•°ï¼ˆsquashing functionï¼‰ï¼š å®ƒå°†èŒƒå›´ï¼ˆ-inf, infï¼‰ä¸­çš„ä»»æ„è¾“å…¥å‹ç¼©åˆ°åŒºé—´ï¼ˆ0, 1ï¼‰ä¸­çš„æŸä¸ªå€¼ï¼š\n\n$$f (x) = \\frac{1}{1+e^{-x}} \\,,$$\n\nåœ¨æœ€æ—©çš„ç¥ç»ç½‘ç»œä¸­ï¼Œç§‘å­¦å®¶ä»¬æ„Ÿå…´è¶£çš„æ˜¯å¯¹â€œæ¿€å‘â€æˆ–â€œä¸æ¿€å‘â€çš„ç”Ÿç‰©ç¥ç»å…ƒè¿›è¡Œå»ºæ¨¡ã€‚ å› æ­¤ï¼Œè¿™ä¸€é¢†åŸŸçš„å…ˆé©±å¯ä»¥ä¸€ç›´è¿½æº¯åˆ°äººå·¥ç¥ç»å…ƒçš„å‘æ˜è€…éº¦å¡æ´›å…‹å’Œçš®èŒ¨ï¼Œä»–ä»¬ä¸“æ³¨äºé˜ˆå€¼å•å…ƒã€‚ é˜ˆå€¼å•å…ƒåœ¨å…¶è¾“å…¥ä½äºæŸä¸ªé˜ˆå€¼æ—¶å–å€¼0ï¼Œå½“è¾“å…¥è¶…è¿‡é˜ˆå€¼æ—¶å–å€¼1ã€‚\n\nå½“äººä»¬é€æ¸å…³æ³¨åˆ°åˆ°åŸºäºæ¢¯åº¦çš„å­¦ä¹ æ—¶ï¼Œ sigmoidå‡½æ•°æ˜¯ä¸€ä¸ªè‡ªç„¶çš„é€‰æ‹©ï¼Œå› ä¸ºå®ƒæ˜¯ä¸€ä¸ªå¹³æ»‘çš„ã€å¯å¾®çš„é˜ˆå€¼å•å…ƒè¿‘ä¼¼ã€‚ å½“æˆ‘ä»¬æƒ³è¦å°†è¾“å‡ºè§†ä½œäºŒå…ƒåˆ†ç±»é—®é¢˜çš„æ¦‚ç‡æ—¶ï¼Œ sigmoidä»ç„¶è¢«å¹¿æ³›ç”¨ä½œè¾“å‡ºå•å…ƒä¸Šçš„æ¿€æ´»å‡½æ•° ï¼ˆä½ å¯ä»¥å°†sigmoidè§†ä¸ºsoftmaxçš„ç‰¹ä¾‹ï¼‰ã€‚ ç„¶è€Œï¼Œsigmoidåœ¨éšè—å±‚ä¸­å·²ç»è¾ƒå°‘ä½¿ç”¨ï¼Œ å®ƒåœ¨å¤§éƒ¨åˆ†æ—¶å€™è¢«æ›´ç®€å•ã€æ›´å®¹æ˜“è®­ç»ƒçš„ReLUæ‰€å–ä»£ã€‚ åœ¨åé¢å…³äºå¾ªç¯ç¥ç»ç½‘ç»œçš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æè¿°åˆ©ç”¨sigmoidå•å…ƒæ¥æ§åˆ¶æ—¶åºä¿¡æ¯æµçš„æ¶æ„ã€‚\n\n\nå½“è¾“å…¥æ¥è¿‘0æ—¶ï¼Œsigmoidå‡½æ•°æ¥è¿‘çº¿æ€§å˜æ¢ã€‚\n\n\n![sigmoid](https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg)\n\nsigmoidå‡½æ•°çš„å¯¼æ•°ä¸ºä¸‹é¢çš„å…¬å¼ï¼š\n\n\n$$\\frac{d}{dx} \\operatorname{sigmoid}(x) = \\frac{\\exp(-x)}{(1 + \\exp(-x))^2} = \\operatorname{sigmoid}(x)\\left(1-\\operatorname{sigmoid}(x)\\right).$$\n\n\n![sigmoid2](http://zh.d2l.ai/_images/output_mlp_76f463_57_0.svg)\n\n\n\n\n\nor a *rectifier*\n$$\\varphi(x) = \\mathrm{max}[0, x] \\,,$$\n![ReLU](https://upload.wikimedia.org/wikipedia/commons/6/6c/Rectifier_and_softplus_functions.svg)\n\nso that the output $O(x_i)$ of the perceptron is given by\n\n$$O (x_i) = \\varphi \\left( \\Sigma_{i=1}^{n} w_i \\, x_i + b   \\right) ,$$\n\nor, in vectorial representation \n\n$$O(x) = \\varphi \\left(\\mathbf{w}^T \\cdot \\mathbf{x} + b   \\right) $$\n\nBelow we present a simple code implementation of the perceptron.\n\n-----\n[1] https://en.wikipedia.org/wiki/Dendrite\n\n[2] https://en.wikipedia.org/wiki/Neuron\n\n[3] https://icwww.epfl.ch/~gerstner/BUCH.html\n\n[4] https://en.wikipedia.org/wiki/Rectifier_(neural_networks)","metadata":{}},{"cell_type":"code","source":"# Define the sigmoid activator; we ask if we want the sigmoid or its derivative\ndef sigmoid_act(x, der=False):\n    import numpy as np\n    \n    if (der==True) : #derivative of the sigmoid\n        f = x/(1-x)\n    else : # sigmoid\n        f = 1/(1+ np.exp(-x))\n    \n    return f\n\n# We may employ the Rectifier Linear Unit (ReLU)\ndef ReLU_act(x, der=False):\n    import numpy as np\n    \n    if (der== True):\n        if x>0 :\n            f= 1\n        else :\n            f = 0\n    else :\n        if x>0:\n            f = x\n        else :\n            f = 0\n    return f\n\n# Now we are ready to define the perceptron; \n# it eats a np.array (that may be a list of features )\ndef perceptron(X, act='Sigmoid'): \n    import numpy as np\n    \n    shapes = X.shape # Pick the number of (rows, columns)!\n    n= shapes[0]+shapes[1]\n    # Generating random weights and bias\n    w = 2*np.random.random(shapes) - 0.5 # We want w to be between -1 and 1\n    b = np.random.random(1)\n    \n    # Initialize the function\n    f = b[0]\n    for i in range(0, X.shape[0]-1) : # run over column elements\n        for j in range(0, X.shape[1]-1) : # run over rows elements\n            f += w[i, j]*X[i,j]/n\n    # Pass it to the activation function and return it as an output\n    if act == 'Sigmoid':\n        output = sigmoid_act(f)\n    else :\n        output = ReLU_act(f)\n        \n    return output\n    ","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Output with sigmoid activator: ', perceptron(features))\nprint('Output with ReLU activator: ', perceptron(features))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n# Define the sigmoid activator; we ask if we want the sigmoid or its derivative\ndef sigmoid_act(x, der=False):\n    import numpy as np\n    \n    if (der==True) : #derivative of the sigmoid\n        f = 1/(1+ np.exp(- x))*(1-1/(1+ np.exp(- x)))\n    else : # sigmoid\n        f = 1/(1+ np.exp(- x))\n    \n    return f\n\n# We may employ the Rectifier Linear Unit (ReLU)\ndef ReLU_act(x, der=False):\n    import numpy as np\n    \n    if (der == True): # the derivative of the ReLU is the Heaviside Theta\n        f = np.heaviside(x, 1)\n    else :\n        f = np.maximum(x, 0)\n    \n    return f","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train/Test split\n\nWe now split the set of features and labels into a training set and a test set.","metadata":{}},{"cell_type":"markdown","source":"# Neural Network's Layer(s)\n\nA standard Artificial Neural Network will be made of multiple layers:\n1. An **Input Layer**, that pass the features to the NN\n2. An arbitrary number of **Hidden Layers**, containing an arbitrary number of neurons for each layer, that receives the inputs and elaborate them. We will introduce Hidden Layers with ReLU activator, since in the *hidden* part of the NN we don't need the output to be contained in the $[0,1]$ range. \n3. An **Output Layer**: these layers contains a number of neurons equal to the number of possible labels we want to have a prediction to; this is because the output of the NN is thus a vector whose dimension is the same as the cardinality of the set of labels, and its entries are the *probability* for each label for the element whose feateures we have passed to the NN. This means that we will use a sigmoid activator to the Output layer, so we squeeze each perceptron's output between 0 and 1. \n\nè¾“å‡ºå±‚**:è¿™äº›å±‚åŒ…å«çš„ç¥ç»å…ƒæ•°é‡ç­‰äºæˆ‘ä»¬æƒ³è¦é¢„æµ‹çš„å¯èƒ½æ ‡ç­¾çš„æ•°é‡;è¿™æ˜¯å› ä¸ºç¥ç»ç½‘ç»œçš„è¾“å‡ºæ˜¯ä¸€ä¸ªå‘é‡ï¼Œå®ƒçš„ç»´æ•°ä¸æ ‡ç­¾é›†çš„åŸºæ•°ç›¸åŒï¼Œå®ƒçš„æ¡ç›®æ˜¯æˆ‘ä»¬å·²ç»ä¼ é€’ç»™ç¥ç»ç½‘ç»œçš„å…ƒç´ çš„æ¯ä¸ªæ ‡ç­¾çš„*æ¦‚ç‡*ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å°†åœ¨Outputå±‚ä½¿ç”¨sigmoidæ¿€æ´»å™¨ï¼Œå› æ­¤æˆ‘ä»¬å°†æ¯ä¸ªæ„ŸçŸ¥å™¨çš„è¾“å‡ºå‹ç¼©åˆ°0å’Œ1ä¹‹é—´ã€‚è¿›å…¥ç¿»è¯‘é¡µé¢\n\n\n\n![ANN](https://miro.medium.com/proxy/1*DW0Ccmj1hZ0OvSXi7Kz5MQ.jpeg)\n\nIn this case, since we have a binary classification (Survived/Perished) we may simply use a single-perceptron Output layer; If the output is smaller than 0.5, the person is perished; otherwise, the person is survived. \n\nFor each layer, we have as an input a matrix made by columns of features (in our example, we have 2 features, i.e. Passenger Class and Passenger Sex), that we label as $I=1,2$. Each of this features will have $n$ entries, so that each feature is a vector $\\{x_I\\}_i$. The layer will have $p$ perceptrons, labelled by $a=1,\\ldots ,p$. Thus the output of the whole layer is a matrix ${O}^{(a)}_{(I)}$ given by\n$${O}^{(a)}_{(I)} = \\varphi\\left( \\mathbf{w}^{(a)}_{(I)} \\cdot  \\mathbf{x}^{(a)}_{(I)} + b^{(a)}_{(I)}   \\right) ,$$\n\nor, explicitly \n$${O}^{(a)}_{(I)} = \\varphi ( {\\large \\Sigma}_{i=1}^{n} \\left({w}^{(a)}_{(I)}\\right)_i \\left(  {x}^{(a)}_{(I)} \\right)_i + \\left( b^{(a)}_{(I)} \\right)_i  )  .$$\n\nIf we start having multiple layers, let us say $N$, we have an additional label $A=1, \\ldots, N$ so that \n$${}^{(A)}{O}^{(a)}_{(I)} = {}^{(A)}\\varphi\\left( {}^{(A)}\\mathbf{w}^{(a)}_{(I)} \\cdot  {}^{(A)}\\mathbf{x}^{(a)}_{(I)} + {}^{(A)}b^{(a)}_{(I)}   \\right) ,$$\n\nwhere we have inserted the label also to the activation function that may depend on which layer we are considering!\n\n## Backpropagation and Gradien Descent\n\nBPç½‘ç»œçš„ç»“æ„é™æ³•çš„åŸºç¡€ä¸Šã€‚BPç½‘ç»œçš„è¾“å…¥è¾“å‡ºå…³ç³»å®è´¨ä¸Šæ˜¯ä¸€ç§æ˜ å°„å…³ç³»ï¼šä¸€ä¸ª è¾“å…¥mè¾“å‡ºçš„BPç¥ç»ç½‘ç»œæ‰€å®Œæˆçš„åŠŸèƒ½æ˜¯ä» ä¸€ç»´æ¬§æ°ç©ºé—´å‘mç»´æ¬§æ°ç©ºé—´ä¸­ä¸€æœ‰é™åŸŸçš„è¿ç»­æ˜ å°„ï¼Œè¿™ä¸€æ˜ å°„å…·æœ‰é«˜åº¦éçº¿æ€§ã€‚å®ƒçš„ä¿¡æ¯å¤„ç†èƒ½åŠ›æ¥æºäºç®€å•éçº¿æ€§å‡½æ•°çš„å¤šæ¬¡å¤åˆï¼Œå› æ­¤å…·æœ‰å¾ˆå¼ºçš„å‡½æ•°å¤ç°èƒ½åŠ›ã€‚è¿™æ˜¯BPç®—æ³•å¾—ä»¥åº”ç”¨çš„åŸºç¡€ã€‚\n\nåå‘ä¼ æ’­ç®—æ³•ä¸»è¦ç”±ä¸¤ä¸ªç¯èŠ‚(æ¿€åŠ±ä¼ æ’­ã€æƒé‡æ›´æ–°)åå¤å¾ªç¯è¿­ä»£ï¼Œç›´åˆ°ç½‘ç»œçš„å¯¹è¾“å…¥çš„å“åº”è¾¾åˆ°é¢„å®šçš„ç›®æ ‡èŒƒå›´ä¸ºæ­¢ã€‚\n\nBPç®—æ³•çš„å­¦ä¹ è¿‡ç¨‹ç”±æ­£å‘ä¼ æ’­è¿‡ç¨‹å’Œåå‘ä¼ æ’­è¿‡ç¨‹ç»„æˆã€‚åœ¨æ­£å‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œè¾“å…¥ä¿¡æ¯é€šè¿‡è¾“å…¥å±‚ç»éšå«å±‚ï¼Œé€å±‚å¤„ç†å¹¶ä¼ å‘è¾“å‡ºå±‚ã€‚å¦‚æœåœ¨è¾“å‡ºå±‚å¾—ä¸åˆ°æœŸæœ›çš„è¾“å‡ºå€¼ï¼Œåˆ™å–è¾“å‡ºä¸æœŸæœ›çš„è¯¯å·®çš„å¹³æ–¹å’Œä½œä¸ºç›®æ ‡å‡½æ•°ï¼Œè½¬å…¥åå‘ä¼ æ’­ï¼Œé€å±‚æ±‚å‡ºç›®æ ‡å‡½æ•°å¯¹å„ç¥ç»å…ƒæƒå€¼çš„åå¯¼æ•°ï¼Œæ„æˆç›®æ ‡å‡½æ•°å¯¹æƒå€¼å‘é‡çš„æ¢¯é‡ï¼Œä½œä¸ºä¿®æ”¹æƒå€¼çš„ä¾æ®ï¼Œç½‘ç»œçš„å­¦ä¹ åœ¨æƒå€¼ä¿®æ”¹è¿‡ç¨‹ä¸­å®Œæˆã€‚è¯¯å·®è¾¾åˆ°æ‰€æœŸæœ›å€¼æ—¶ï¼Œç½‘ç»œå­¦ä¹ ç»“æŸã€‚\n\n\né€šè¿‡è®¡ç®—æ¢¯åº¦ä½¿ä»£ä»·å‡½æ•°æœ€å°åŒ–ï¼Œå¾—åˆ°å±€éƒ¨æå°å€¼ã€‚åœ¨æ¯ä¸ªè¿­ä»£æˆ–è®­ç»ƒæ­¥éª¤ä¸­ï¼Œç½‘ç»œä¸­çš„æƒé‡é€šè¿‡è®¡ç®—çš„æ¢¯åº¦ä»¥åŠ learning rate è¿›è¡Œæ›´æ–°ï¼Œè¯¥æ¢¯åº¦æ§åˆ¶å¯¹æƒé‡å€¼çš„ä¿®æ”¹å› å­ã€‚åœ¨ç¥ç»ç½‘ç»œçš„è®­ç»ƒé˜¶æ®µï¼Œæ¯ä¸ªæ­¥éª¤éƒ½è¦é‡å¤è¿™ä¸ªè¿‡ç¨‹ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œç›®æ ‡æ˜¯åœ¨æ¯ä¸€æ­¥ä¹‹åæ›´æ¥è¿‘å±€éƒ¨æœ€å°å€¼ã€‚\n\n\n\n\n\n\n\nFor adjusting the trainable parameters $\\{w\\}$ and $\\{b\\}$, we need to implement the *backpropagation*. We want to minimize a certain **cost function**\n$$\\mu(y,\\bar{y})=|y-\\bar{y}|^2$$\nwhere $y$ is the output of the output layer while $\\bar{y}$ is the actual label; in order to do so, we start the *gradient descent*, which means that we see the cost function as a function of the trainable parameters $\\mathbf{w}$ such as $\\{w\\}$ and $\\{b\\}$, we compute the gradient - gradient that can be seen as the slope of the multidimensional graph [3] - and we subtract it from the randomly initialized set, as\n$$\\mathbf{w}'_n =  \\mathbf{w}_n - \\eta \\nabla \\mu(\\mathbf{w}_n)  \\,,$$\nmoving thus towards the *optima*, or global minimum, of the cost function. In the formula above $\\eta$ is the **learning rate** of the ANN [4] \n\n\nå¦‚ä¸‹å›¾æ‰€ç¤º [5]:\n\n![GD](https://hackernoon.com/hn-images/1*f9a162GhpMbiTVTAua_lLQ.png)\n\n\n\n\n\n\n### å›é¡¾\n\nWe now recap what we have shown: \n* **Input**: è®¾ç½®è¾“å…¥ $z_0^{i_0}$;\n* **Feed Forward**: è®¡ç®—$\\alpha$-th å±‚ $z_\\alpha^{i_\\alpha}$ \n$$z_\\alpha^{i_\\alpha} = \\varphi ( w_{\\alpha}^{i_{\\alpha} i_{\\alpha-1}} \\, z^{i_{\\alpha-1}}_{\\alpha-1} + b_\\alpha^{i_\\alpha} ) \\equiv \\varphi (\\zeta^{i_\\alpha}_\\alpha) ,$$\nç›´åˆ°è¾“å‡ºå±‚, åˆ©ç”¨çš„å…¬å¼ä¸º\n$$y = f( w_{Out}^{i_n} z_n^{i_n} + b_{Out} ) .$$\n* **è®¡ç®—åå·®**: é€šè¿‡å…¬å¼è®¡ç®—æœ€åä¸€å±‚çš„è¯¯å·®\n$$\\delta^N_{i_N} = \\frac{\\partial \\mu}{\\partial z_N^{i_N} } \\,.$$\næœ€åä¸€å±‚å®é™…ä¸Šæ˜¯è¾“å‡ºå±‚ï¼Œæ‰€ä»¥\n$$\\delta^{Out} = \\frac{\\partial \\mu}{\\partial z_{Out} } \\cdot f' (z_{Out}) .$$\n*  **Backpropagate the Error**: å¯¹æ¯ä¸€å±‚ $\\alpha= N-1, \\ldots, 2$ è®¡ç®— \n$$\\delta_{\\alpha}^{i_\\alpha}  = {\\large \\Sigma}_{{i_\\alpha}}  \\delta_{\\alpha+1}^{i_\\alpha+1}  \\, w_{\\alpha}^{i_{\\alpha+1} i_\\alpha} \\, \\varphi' (z_{\\alpha}^{i_\\alpha}) \\,.$$\n* **Output**: æŸå¤±å‡½æ•°çš„æ¢¯åº¦è®¡ç®—\n$$\\frac{\\partial \\mu}{\\partial w_\\alpha^{i_{\\alpha+1} i_\\alpha} } = \\delta_\\alpha^{i_\\alpha} \\cdot z_{\\alpha-1}^{i_\\alpha-1} \\varphi'(\\zeta_\\alpha^{i_\\alpha} ) ,$$\n$$\\frac{\\partial \\mu}{\\partial b_\\alpha^{i_{\\alpha}} } =  \\delta_\\alpha^{i_\\alpha} \\cdot  \\varphi'(\\zeta_\\alpha^{i_\\alpha} ) . $$\n\næ¿€æ´»å‡½æ•°çš„æ˜¾å¼å¯¼æ•°æ˜¯å·²çŸ¥çš„ï¼Œå¾ˆå®¹æ˜“è®¡ç®—ï¼Œå› ä¸º\n$$f'(x) = f(x)(1-f(x)) = y(1-y) \\,,$$\nwhile \n$$\\varphi'(x) = \\Theta(x)\\,,$$\nwhere $\\Theta(x)$ is the *Heaviside Theta*, that outputs 1 for $x\\ge 0$ and $0$ otherwise. \n\n\n\n\n## Our ANN\n \nLet us work out the example for our Neural Network: we will initialize an Artificial Neural Network having\n1. An **input Layer** whose inputs is a 2-entris vector $x_i$ = $[$ Passenger Class, Sex $]$. \n\n2. **Two Hidden Layers** with respectively $p$ and $q$ neurons *each*, with ReLU activator $\\varphi$, whose outpus are $z_1^{j}$ and $z_2^{k}$ and whose parameters are $\\{ w_1^{j i}, b_1^j\\}$ and $\\{ w_2^{kj}, b_2^k\\}$;\n\n3. å› ä¸ºæˆ‘ä»¬è¦å¯¹ä¸‰ç§èŠ±è‰²è¿›è¡Œåˆ†ç±»ï¼Œæ‰€ä»¥è¾“å‡ºå±‚ä¸º3ä¸ªèŠ‚ç‚¹\nThis ANN look somewhat similar to the picture below.\n\n![HL](http://cs231n.github.io/assets/nn1/neural_net2.jpeg)\n\nThis means that our algorithms will have to perform the following steps:\n0. After a train/test split of both training data $\\{x_i\\}_I$ and labels $\\{\\bar{y}\\}_I$, where $I$ runs over the passengers, it has to reassamble the train data into batches to train our Neural Network. For sake of simplicity, and since we are employing *batch gradient descent* [7], we will use the whole training dataset as a batch and thus have only 1 epoch. \n1. Inputs the training data $\\{x_i\\}_I$ and feed them to the first Hidden Layer;\n2. Initialize randomly all the parameters for each layers, i.e. $\\{ w_1^{j i}, b_1^j\\}$, $\\{ w_2^{kj}, b_2^k\\}$ and $\\{ w_{Out}^k, b_{Out}\\}$. Notice that $i=0,1$, $j=0, \\ldots, p-1$ and $k=0, \\ldots, q-1$. Now we can perform the following steps: For each $I$ in the passenger list, we have to\n    1. **Feedforward**: compute *in this order*\n    $$z_1^j = \\varphi( {\\large \\Sigma}_{i=0,1} \\, w_1^{j i} x^i + b_1^j ), $$\n    $$z_2^k = \\varphi( {\\large \\Sigma}_{j=0,\\ldots, p-1} \\, w_2^{k j} z_1^j + b_2^k ), $$\n    $$y = f( {\\large \\Sigma}_{k=0, \\ldots, q-1} w_{Out}^k z_2^k + b_{Out} ) .$$\n    2.  **Compute the outer layer Error**: Compute the vector\n    $$\\delta_{Out} = \\frac{\\partial \\mu}{\\partial y} \\cdot f' (y) = 2(y-\\bar{y}) \\cdot y(1-y) $$\n    3. **Backpropagate the error**: Compute *in this order* the following\n    $$\\delta_2^{k} =   \\delta_{Out}  \\, w_{Out}^{k} \\, \\varphi' (z_{2}^{k}) ,$$\n    $$\\delta_1^j = {\\large \\Sigma}_{k=0, \\ldots, q-1} \\delta_2^{k} w_2^{k j}  \\, \\varphi' (z_{1}^{j}) . $$\n3. **Gradient descent**: When we pass from the $I$-th to the $(I+1)$-th passenger in the training set, we need to update the weights *in this order* following the rules\n    * Output Layer:\n    $${}^{(I+1)}w_{Out}^k = {}^{(I)} w_{Out}^k - \\eta {}^{(I)}\\delta_{Out} \\, z_2^k \\,,$$\n    $${}^{(I+1)}b_{Out} = {}^{(I)} b_{Out} - \\eta {}^{(I)}\\delta_{Out}\\,,$$\n    * Second Layer:\n    $${}^{(I+1)}w_{2}^{kj} = {}^{(I)} w_{2}^{kj} - \\eta \\delta_2^{k} \\, z_1^{j} \\,,$$\n    $${}^{(I+1)}b_{2}^k = {}^{(I)} b_{2}^k - \\eta \\delta_2^{k} \\,, $$\n    * First Layer:\n    $${}^{(I+1)}w_{1}^{ji} = {}^{(I)} w_{1}^{ji} - \\eta \\delta_1^{j} \\, x_i^{i} \\,,$$\n    $${}^{(I+1)}b_{1} = {}^{(I)} b_{1}^j - \\eta \\delta_2^{j} \\,, $$\n\nNow we can iterate over the whole set $I=0, \\ldots \\, \\#$training_set_passengers so that we can **train** our Neural Network!\n\n-------\n[1] http://cs231n.github.io/neural-networks-1/\n\n[2] https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/\n\n[3] https://www.nature.com/articles/323533a0 (original paper) https://en.wikipedia.org/wiki/Gradient_descent (Wiki article)\n\n[4] https://en.wikipedia.org/wiki/Stochastic_gradient_descent\n\n[5] https://hackernoon.com/gradient-descent-aynk-7cbe95a778da\n\n[6] http://neuralnetworksanddeeplearning.com/chap2.html\n\n[7] https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/\n\n[Picture 1] https://medium.com/datadriveninvestor/when-not-to-use-neural-networks-89fb50622429","metadata":{}},{"cell_type":"code","source":"# split into training and testing sets\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(features, labels, test_size=0.30)\n\nprint('Training records:',Y_train.size)\nprint('Test records:',Y_test.size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training the model","metadata":{}},{"cell_type":"code","source":"# Set up the number of perceptron per each layer:\np=4 # Layer 1\nq=4 # Layer 2\n\n# Set up the Learning rate\neta =  1/623\n\n\n# 0: Random initialize the relevant data \nw1 = 2*np.random.rand(p , X_train.shape[1]) - 0.5 # Layer 1\nb1 = np.random.rand(p)\n\nw2 = 2*np.random.rand(q , p) - 0.5  # Layer 2\nb2 = np.random.rand(q)\n\nwOut = 2*np.random.rand(q) - 0.5  # Output Layer\nbOut = np.random.rand(1)\n\nmu = []\nvec_y = []\n\n# Start looping over the passengers, i.e. over I.\n\nfor I in range(0, X_train.shape[0]): #loop in all the passengers:\n    \n    # 1: input the data \n    x = X_train[I]\n    \n    \n    # 2: Start the algorithm\n    \n    # 2.1: Feed forward\n    z1 = ReLU_act(np.dot(w1, x) + b1) # output layer 1 \n    z2 = ReLU_act(np.dot(w2, z1) + b2) # output layer 2\n    y = sigmoid_act(np.dot(wOut, z2) + bOut) # Output of the Output layer\n    \n    #2.2: Compute the output layer's error\n    delta_Out =  (y-Y_train[I]) * sigmoid_act(y, der=True)\n    \n    #2.3: Backpropagate\n    delta_2 = delta_Out * wOut * ReLU_act(z2, der=True) # Second Layer Error\n    delta_1 = np.dot(delta_2, w2) * ReLU_act(z1, der=True) # First Layer Error\n    \n    # 3: Gradient descent \n    wOut = wOut - eta*delta_Out*z2  # Outer Layer\n    bOut = bOut - eta*delta_Out\n    \n    w2 = w2 - eta*np.kron(delta_2, z1).reshape(q,p) # Hidden Layer 2\n    b2 = b2 - eta*delta_2\n    \n    w1 = w1 - eta*np.kron(delta_1, x).reshape(p, x.shape[0]) # Hidden Layer 1\n    b1 = b1 - eta*delta_1\n    \n    # 4. Computation of the loss function\n    mu.append((1/2)*(y-Y_train[I])**2)\n    vec_y.append(y[0])\n\n\n# Plotting the Cost function for each training data     \nplt.figure(figsize=(10,6))\nplt.scatter(np.arange(0, X_train.shape[0]), mu, alpha=0.3, s=4, label='mu')\nplt.title('Loss for each training data point', fontsize=20)\nplt.xlabel('Training data', fontsize=16)\nplt.ylabel('Loss', fontsize=16)\nplt.show()\n\n# Plotting the average cost function over 10 training data    \npino = []\nfor i in range(0, 9):\n    pippo = 0\n    for m in range(0, 59):\n        pippo+=vec_y[60*i+m]/60\n    pino.append(pippo)\n    \n    \n\nplt.figure(figsize=(10,6))\nplt.scatter(np.arange(0, 9), pino, alpha=1, s=10, label='error')\nplt.title('Averege Loss by epoch', fontsize=20)\nplt.xlabel('Epoch', fontsize=16)\nplt.ylabel('Loss', fontsize=16)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define the Training as a Function\n\nSince we have learnt how to train an ANN, we are in the position of define a function that does that, by eating the X_train, Y_train as well as the number of perceptron $p,q$ for the first and second hidden layer, and the learning rate $\\eta$.","metadata":{}},{"cell_type":"code","source":"def ANN_train(X_train, Y_train, p=4, q=4, eta=0.0015):\n    import numpy as np\n    import matplotlib.pyplot as plt\n    \n    # 0: Random initialize the relevant data \n    w1 = 2*np.random.rand(p , X_train.shape[1]) - 0.5 # Layer 1\n    b1 = np.random.rand(p)\n\n    w2 = 2*np.random.rand(q , p) - 0.5  # Layer 2\n    b2 = np.random.rand(q)\n\n    wOut = 2*np.random.rand(q) - 0.5   # Output Layer\n    bOut = np.random.rand(1)\n\n    mu = []\n    vec_y = []\n\n    # Start looping over the passengers, i.e. over I.\n\n    for I in range(0, X_train.shape[0]-1): #loop in all the passengers:\n    \n        # 1: input the data \n        x = X_train[I]\n    \n        # 2: Start the algorithm\n    \n        # 2.1: Feed forward\n        z1 = ReLU_act(np.dot(w1, x) + b1) # output layer 1 \n        z2 = ReLU_act(np.dot(w2, z1) + b2) # output layer 2\n        y = sigmoid_act(np.dot(wOut, z2) + bOut) # Output of the Output layer\n    \n        #2.2: Compute the output layer's error\n        delta_Out = 2 * (y-Y_train[I]) * sigmoid_act(y, der=True)\n    \n        #2.3: Backpropagate\n        delta_2 = delta_Out * wOut * ReLU_act(z2, der=True) # Second Layer Error\n        delta_1 = np.dot(delta_2, w2) * ReLU_act(z1, der=True) # First Layer Error\n    \n        # 3: Gradient descent \n        wOut = wOut - eta*delta_Out*z2  # Outer Layer\n        bOut = bOut - eta*delta_Out\n    \n        w2 = w2 - eta*np.kron(delta_2, z1).reshape(q,p) # Hidden Layer 2\n        b2 = b2 -  eta*delta_2\n    \n        w1 = w1 - eta*np.kron(delta_1, x).reshape(p, x.shape[0])\n        b1 = b1 - eta*delta_1\n    \n        # 4. Computation of the loss function\n        mu.append((y-Y_train[I])**2)\n        vec_y.append(y)\n    \n    batch_loss = []\n    for i in range(0, 10):\n        loss_avg = 0\n        for m in range(0, 60):\n            loss_avg+=vec_y[60*i+m]/60\n        batch_loss.append(loss_avg)\n    \n    \n    plt.figure(figsize=(10,6))\n    plt.scatter(np.arange(1, len(batch_loss)+1), batch_loss, alpha=1, s=10, label='error')\n    plt.title('Averege Loss by epoch', fontsize=20)\n    plt.xlabel('Epoch', fontsize=16)\n    plt.ylabel('Loss', fontsize=16)\n    plt.show()\n    \n    return w1, b1, w2, b2, wOut, bOut, mu","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w1, b1, w2, b2, wOut, bOut, mu = ANN_train(X_train, Y_train, p=8, q=4, eta=0.0015)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Compute Predictions\n\nWe now have to compute predictions from our trained ANN. In order to do so we need to recall the trained parameters $\\{w\\}, \\{b\\}$ and use them to actually get the predictions. ","metadata":{}},{"cell_type":"code","source":"def ANN_pred(X_test, w1, b1, w2, b2, wOut, bOut, mu):\n    import numpy as np\n    \n    pred = []\n    \n    for I in range(0, X_test.shape[0]): #loop in all the passengers\n        # 1: input the data \n        x = X_test[I]\n        \n        # 2.1: Feed forward\n        z1 = ReLU_act(np.dot(w1, x) + b1) # output layer 1 \n        z2 = ReLU_act(np.dot(w2, z1) + b2) # output layer 2\n        y = sigmoid_act(np.dot(wOut, z2) + bOut)  # Output of the Output layer\n        \n        # Append the prediction;\n        # We now need a binary classifier; we this apply an Heaviside Theta and we set to 0.5 the threshold\n        # if y < 0.5 the output is zero, otherwise is 1\n        pred.append( np.heaviside(y - 0.5, 1)[0] )\n    \n    \n    return np.array(pred);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = ANN_pred(X_test, w1, b1, w2, b2, wOut, bOut, mu)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation report","metadata":{}},{"cell_type":"code","source":"# Plot the confusion matrix\ncm = confusion_matrix(Y_test, predictions)\n\ndf_cm = pd.DataFrame(cm, index = [dict_live[i] for i in range(0,2)], columns = [dict_live[i] for i in range(0,2)])\nplt.figure(figsize = (7,7))\nsns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues, fmt='g')\nplt.xlabel(\"Predicted Class\", fontsize=18)\nplt.ylabel(\"True Class\", fontsize=18)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Of course the outcome is **not** amazing, but we have to keep in mind that this is a very simple (almost naive) implementation of an artificial neural network whose main goal was to explain (mostly to myself) how an ANN works in practice and how it is possible to implement it into *Python* from scratch. ","metadata":{}},{"cell_type":"markdown","source":"# Exporting the predictions and submit them\n\nWe now need to load the 'test.csv' dataset and use the trained ANN to make predictions. ","metadata":{}},{"cell_type":"code","source":"test_data = pd.read_csv('../input/titanic/test.csv')\n\ntest_data.head(4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We extract the relevant feature from the test_data as we have done before","metadata":{}},{"cell_type":"code","source":"# We apply the dictionary using a lambda function and the pandas .apply() module\ntest_data['Bsex'] = test_data['Sex'].apply(lambda x : dict_sex[x])\n\n\nX = test_data[['Pclass', 'Bsex']].to_numpy()\n\ntest_predictions = ANN_pred(X, w1, b1, w2, b2, wOut, bOut, mu)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We thus export the predictions as a *Comma Separated Values* (csv) file. We then create a link that allows us to download such csv [1].\n\n-----\n[1] https://www.kaggle.com/getting-started/58426","metadata":{}},{"cell_type":"code","source":"submission = pd.DataFrame({\n        \"PassengerId\": test_data[\"PassengerId\"],\n        \"Survived\": test_predictions\n    })\n\nsubmission.head(5)\n\n# Export it in a 'Comma Separated Values' (CSV) file\nimport os\nos.chdir(r'../working')\nsubmission.to_csv(r'submission.csv', index=False)\n# Creating a link to download the .csv file we created\nfrom IPython.display import FileLink\nFileLink(r'submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have thus created a csv file containing our predictions for the Titanic 'test.csv' dataframe using our simple (2+1)-layer Artificial Neural Network!","metadata":{}},{"cell_type":"markdown","source":"# The ANN as a Class\n\nWe want now to implement the code as Class for Python, so that we could easiliy generalize it; the goal is to have a class from which we can instantiate an object \"Neural Network\", and add to it as many hidden layers with as many neurons we want, with the desired activation functions and so on. To generalize further, we define also the *Activation_function* class and the *layers* class, so we may easily add more activation funtions or more different layers (such as Convolutional or Pooling layers for Convolutional Neural Networks).\n\nWithin the ANN class, we define the following methods:\n* **add**: it eats a tuple ( int(number_of_neurons), string(activation_function) ), i.e. the output of the ANN.layer method. It is a void method. It updates the HiddenLayer string defined by the __init__ method. \n* **FeedForward**: it implements the Feed Forward layer by layer.\n* **BackPropagation**: it implements the whole gradient descent mechanism; first, it computes the errors by implementing the backpropagation; then, it updates the ANN parameters by gradient descent. \n* **Fit**: this method eats the training features and labels and fits the ANN by calling iteratively *FeedForward* and *BackPropagation* methods. This allow us to easily modify (or generalize) either *FeedForward* or *BackPropagation* methods without altering the *Fit* method.\n* **predict**: it eats the featurs and spits the label predictions. \n* **set_learning_rate**: by default the learning rate is initialized to be 1, but we can call this method to set it to a different value. \n* **get_accuracy, get_avg_accuracy**: these methods' aim is to return the cost function either at each step of the training process or averaging over 10 passengers, respectively.\n\nIn the **layers** class we have\n* **layer**: it eats two imputs, the number of neurons and the activation function (as a string); it returns a tuple of the two. The idea is to leave room for a generalization of the *layers.layer* method later on by adding multiple layer type (i.e. Pooling or Convolutional layers for the CNN).\n\nIn the **Activation_function** class we have\n* **ReLU_act, sigmoid_act**: these are the activation functions. They can be easily generalized (LeakyReLU, ParametricReLU, etc.)","metadata":{}},{"cell_type":"markdown","source":"## An initialization Improvement\n\nI recently found an interesting article on the web [1] (see [2] for an easy&fast review) where the authors were able to define a well-performing initialization method for the $\\{w\\}$ and $\\{b\\}$ of the ANN. It relies on the knowledge of the dimension of the previous layer; so they are gaussian-distributed randomly initialized values divided by the squareroot of the dimension of the previous layer, i.e.\n> w = np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(2/layer_size[l-1])\n>\n> b = np.random.randn(layer_size[l])*np.sqrt(2/layer_size[l-1])\n\nUsing that, as well as *Parametric Rectified Linear Units* (P-ReLU), they were able to obtain a result that\n> [...] *is the first to surpass human-level performance (5.1%, Russakovsky et al.) on this visual recognition challenge.*\n\nThe visual recognition challenge they are referring to is the **ImageNet Large Scale Visual Recognition Competition (ILSVRC) 2014**.\n\n-----\n[1] https://arxiv.org/abs/1502.01852\n\n[2] https://towardsdatascience.com/random-initialization-for-neural-networks-a-thing-of-the-past-bfcdd806bf9e","metadata":{}},{"cell_type":"code","source":"'''\nArtificial Neural Network Class\n'''\nclass ANN:\n    import numpy as np # linear algebra\n    np.random.seed(10)\n    \n    '''\n    Initialize the ANN;\n    HiddenLayer vector : will contain the Layers' info\n    w, b, phi = (empty) arrays that will contain all the w, b and activation functions for all the Layers\n    mu = cost function\n    eta = a standard learning rate initialization. It can be modified by the 'set_learning_rate' method\n    '''\n    def __init__(self) :\n        self.HiddenLayer = []\n        self.w = []\n        self.b = []\n        self.phi = []\n        self.mu = []\n        self.eta = 1 #set up the proper Learning Rate!!\n    \n    '''\n    add method: to add layers to the network\n    '''\n    def add(self, lay = (4, 'ReLU') ):\n        self.HiddenLayer.append(lay)\n    \n    '''\n    FeedForward method: as explained before. \n    '''\n    @staticmethod\n    def FeedForward(w, b, phi, x):\n        return phi(np.dot(w, x) + b)\n        \n    '''\n    BackPropagation algorithm implementing the Gradient Descent \n    '''\n    def BackPropagation(self, x, z, Y, w, b, phi):\n        self.delta = []\n        \n        # We initialize ausiliar w and b that are used only inside the backpropagation algorithm once called        \n        self.W = []\n        self.B = []\n        \n        # We start computing the LAST error, the one for the OutPut Layer \n        self.delta.append(  (z[len(z)-1] - Y) * phi[len(z)-1](z[len(z)-1], der=True) )\n        \n        '''Now we BACKpropagate'''\n        # We thus compute from next-to-last to first\n        for i in range(0, len(z)-1):\n            self.delta.append( np.dot( self.delta[i], w[len(z)- 1 - i] ) * phi[len(z)- 2 - i](z[len(z)- 2 - i], der=True) )\n        \n        # We have the error array ordered from last to first; we flip it to order it from first to last\n        self.delta = np.flip(self.delta, 0)  \n        \n        # Now we define the delta as the error divided by the number of training samples\n        self.delta = self.delta/self.X.shape[0] \n        \n        '''GRADIENT DESCENT'''\n        # We start from the first layer that is special, since it is connected to the Input Layer\n        self.W.append( w[0] - self.eta * np.kron(self.delta[0], x).reshape( len(z[0]), x.shape[0] ) )\n        self.B.append( b[0] - self.eta * self.delta[0] )\n        \n        # We now descend for all the other Hidden Layers + OutPut Layer\n        for i in range(1, len(z)):\n            self.W.append( w[i] - self.eta * np.kron(self.delta[i], z[i-1]).reshape(len(z[i]), len(z[i-1])) )\n            self.B.append( b[i] - self.eta * self.delta[i] )\n        \n        # We return the descended parameters w, b\n        return np.array(self.W), np.array(self.B)\n    \n    \n    '''\n    Fit method: it calls FeedForward and Backpropagation methods\n    '''\n    def Fit(self, X_train, Y_train):            \n        print('Start fitting...')\n        '''\n        Input layer\n        '''\n        self.X = X_train\n        self.Y = Y_train\n        \n        '''\n        We now initialize the Network by retrieving the Hidden Layers and concatenating them \n        '''\n        print('Model recap: \\n')\n        print('You are fitting an ANN with the following amount of layers: ', len(self.HiddenLayer))\n        \n        for i in range(0, len(self.HiddenLayer)) :\n            print('Layer ', i+1)\n            print('Number of neurons: ', self.HiddenLayer[i][0])\n            if i==0:\n                # We now try to use the He et al. Initialization from ArXiv:1502.01852\n                self.w.append( np.random.randn(self.HiddenLayer[i][0] , self.X.shape[1])/np.sqrt(2/self.X.shape[1]) )\n                self.b.append( np.random.randn(self.HiddenLayer[i][0])/np.sqrt(2/self.X.shape[1]))\n                # Old initialization\n                #self.w.append(2 * np.random.rand(self.HiddenLayer[i][0] , self.X.shape[1]) - 0.5)\n                #self.b.append(np.random.rand(self.HiddenLayer[i][0]))\n                \n                # Initialize the Activation function\n                for act in Activation_function.list_act():\n                    if self.HiddenLayer[i][1] == act :\n                        self.phi.append(Activation_function.get_act(act))\n                        print('\\tActivation: ', act)\n\n            else :\n                # We now try to use the He et al. Initialization from ArXiv:1502.01852\n                self.w.append( np.random.randn(self.HiddenLayer[i][0] , self.HiddenLayer[i-1][0] )/np.sqrt(2/self.HiddenLayer[i-1][0]))\n                self.b.append( np.random.randn(self.HiddenLayer[i][0])/np.sqrt(2/self.HiddenLayer[i-1][0]))\n                # Old initialization\n                #self.w.append(2*np.random.rand(self.HiddenLayer[i][0] , self.HiddenLayer[i-1][0] ) - 0.5)\n                #self.b.append(np.random.rand(self.HiddenLayer[i][0]))\n                \n                # Initialize the Activation function\n                for act in Activation_function.list_act():\n                    if self.HiddenLayer[i][1] == act :\n                        self.phi.append(Activation_function.get_act(act))\n                        print('\\tActivation: ', act)\n            \n        '''\n        Now we start the Loop over the training dataset\n        '''  \n        for I in range(0, self.X.shape[0]): # loop over the training set\n            '''\n            Now we start the feed forward\n            '''  \n            self.z = []\n            \n            self.z.append( self.FeedForward(self.w[0], self.b[0], self.phi[0], self.X[I]) ) # First layers\n            \n            for i in range(1, len(self.HiddenLayer)): #Looping over layers\n                self.z.append( self.FeedForward(self.w[i] , self.b[i], self.phi[i], self.z[i-1] ) )\n        \n            \n            '''\n            Here we backpropagate\n            '''      \n            self.w, self.b  = self.BackPropagation(self.X[I], self.z, self.Y[I], self.w, self.b, self.phi)\n            \n            '''\n            Compute cost function\n            ''' \n            self.mu.append(\n                (1/2) * np.dot(self.z[len(self.z)-1] - self.Y[I], self.z[len(self.z)-1] - self.Y[I]) \n            )\n            \n        print('Fit done. \\n')\n        \n\n    \n    '''\n    predict method\n    '''\n    def predict(self, X_test):\n        \n        print('Starting predictions...')\n        \n        self.pred = []\n        self.XX = X_test\n        \n        for I in range(0, self.XX.shape[0]): # loop over the training set\n            \n            '''\n            Now we start the feed forward\n            '''  \n            self.z = []\n            \n            self.z.append(self.FeedForward(self.w[0] , self.b[0], self.phi[0], self.XX[I])) #First layer\n    \n            for i in range(1, len(self.HiddenLayer)) : # loop over the layers\n                self.z.append( self.FeedForward(self.w[i] , self.b[i], self.phi[i], self.z[i-1]))\n       \n            # Append the prediction;\n            # We now need a binary classifier; we this apply an Heaviside Theta and we set to 0.5 the threshold\n            # if y < 0.5 the output is zero, otherwise is zero\n            self.pred.append( np.heaviside(  self.z[-1] - 0.5, 1)[0] ) # NB: self.z[-1]  is the last element of the self.z list\n        \n        print('Predictions done. \\n')\n\n        return np.array(self.pred)\n   \n    '''\n    We need a method to retrieve the accuracy for each training data to follow the learning of the ANN\n    '''\n    def get_accuracy(self):\n        return np.array(self.mu)\n    # This is the averaged version\n    def get_avg_accuracy(self):\n        import math\n        self.batch_loss = []\n        for i in range(0, 10):\n            self.loss_avg = 0\n            # To set the batch in 10 element/batch we use math.ceil method\n            # int(math.ceil((self.X.shape[0]-10) / 10.0))    - 1\n            for m in range(0, (int(math.ceil((self.X.shape[0]-10) / 10.0))   )-1):\n                #self.loss_avg += self.mu[60*i+m]/60\n                self.loss_avg += self.mu[(int(math.ceil((self.X.shape[0]-10) / 10.0)) )*i + m]/(int(math.ceil((self.X.shape[0]-10) / 10.0)) )\n            self.batch_loss.append(self.loss_avg)\n        return np.array(self.batch_loss)\n    \n    '''\n    Method to set the learning rate\n    '''\n    def set_learning_rate(self, et=1):\n        self.eta = et\n        \n        \n'''\nlayers class\n'''\nclass layers :\n    '''\n    Layer method: used to call standar layers to add. \n    Easily generalizable to more general layers (Pooling and Convolutional layers)\n    '''        \n    def layer(p=4, activation = 'ReLU'):\n        return (p, activation)\n\n'''\nActivation functions class\n'''\nclass Activation_function(ANN):\n    import numpy as np\n    \n    def __init__(self) :\n        super().__init__()\n        \n    '''\n    Define the sigmoid activator; we ask if we want the sigmoid or its derivative\n    '''\n    def sigmoid_act(x, der=False):\n        if (der==True) : #derivative of the sigmoid\n            f = 1/(1+ np.exp(- x))*(1-1/(1+ np.exp(- x)))\n        else : # sigmoid\n            f = 1/(1+ np.exp(- x))\n        return f\n\n    '''\n    Define the Rectifier Linear Unit (ReLU)\n    '''\n    def ReLU_act(x, der=False):\n        if (der == True): # the derivative of the ReLU is the Heaviside Theta\n            f = np.heaviside(x, 1)\n        else :\n            f = np.maximum(x, 0)\n        return f\n    \n    def list_act():\n        return ['sigmoid', 'ReLU']\n    \n    def get_act(string = 'ReLU'):\n        if string == 'ReLU':\n            return ReLU_act\n        elif string == 'sigmoid':\n            return sigmoid_act\n        else :\n            return sigmoid_act","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Instantiating the class and Fitting the model\n\nNow we instantiate our model, that will be exactly the same as before, i.e. a two-hidden layer with 8 and 4 neurons respectively and with ReLU activation plus an OutPut layer with a single neuron with sigmoid activation. \n\nWe will set the learning rate and then fit the model; after that we recover the accuracy history (even in the averaged over 10 batches) and finally we compute the predictions.","metadata":{}},{"cell_type":"code","source":"model = ANN()\n\nmodel.add(layers.layer(8, 'ReLU'))\nmodel.add(layers.layer(4, 'ReLU'))\nmodel.add(layers.layer(1, 'sigmoid'))\n\nmodel.set_learning_rate(0.8)\n\nmodel.Fit(X_train, Y_train)\nacc_val = model.get_accuracy()\nacc_avg_val = model.get_avg_accuracy()\n\npredictions = model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We plot the accuracy stories we have retrieved:","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nplt.scatter(np.arange(1, X_train.shape[0]+1), acc_val, alpha=0.3, s=4, label='mu')\nplt.title('Loss for each training data point', fontsize=20)\nplt.xlabel('Training data', fontsize=16)\nplt.ylabel('Loss', fontsize=16)\nplt.show()\n\nplt.figure(figsize=(10,6))\nplt.scatter(np.arange(1, len(acc_avg_val)+1), acc_avg_val, label='mu')\nplt.title('Averege Loss by epoch', fontsize=20)\nplt.xlabel('Training data', fontsize=16)\nplt.ylabel('Loss', fontsize=16)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And finally see how the system behaves as a classifier:","metadata":{}},{"cell_type":"code","source":"# Plot the confusion matrix\ncm = confusion_matrix(Y_test, predictions)\n\ndf_cm = pd.DataFrame(cm, index = [dict_live[i] for i in range(0,2)], columns = [dict_live[i] for i in range(0,2)])\nplt.figure(figsize = (7,7))\nsns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues, fmt='g')\nplt.xlabel(\"Predicted Class\", fontsize=18)\nplt.ylabel(\"True Class\", fontsize=18)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusions\nNotice that, having changed the random initialization following the He et al. procedure, we have improved the classification slightly (recall that this still is a very naive model). Now we can easily add more layers and, in the spirit of Convolutional Neural Network, we can easily modify the class to add for more generic layers like *Convolutional* or *Pooling* layers.\n\nWe can now try a more *deep* neural network for the case at hand:","metadata":{}},{"cell_type":"code","source":"model = ANN()\n\nmodel.add(layers.layer(24, 'ReLU'))\nmodel.add(layers.layer(12, 'sigmoid'))\nmodel.add(layers.layer(6, 'ReLU'))\nmodel.add(layers.layer(1, 'sigmoid'))\n\nmodel.set_learning_rate(0.8)\n\nmodel.Fit(X_train, Y_train)\nacc_val = model.get_accuracy()\nacc_avg_val = model.get_avg_accuracy()\n\nplt.figure(figsize=(10,6))\nplt.scatter(np.arange(1, len(acc_avg_val)+1), acc_avg_val, label='mu')\nplt.title('Averege Loss by epoch', fontsize=20)\nplt.xlabel('Training data', fontsize=16)\nplt.ylabel('Loss', fontsize=16)\nplt.show()\n\npredictions = model.predict(X_test)\n\n# Plot the confusion matrix\ncm = confusion_matrix(Y_test, predictions)\n\ndf_cm = pd.DataFrame(cm, index = [dict_live[i] for i in range(0,2)], columns = [dict_live[i] for i in range(0,2)])\nplt.figure(figsize = (7,7))\nsns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues, fmt='g')\nplt.xlabel(\"Predicted Class\", fontsize=18)\nplt.ylabel(\"True Class\", fontsize=18)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I hope you enjoyed it!\n\nI would love to receive your comments (good AND bad), so feel free to leave a comment! ","metadata":{}}]}