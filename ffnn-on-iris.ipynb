{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Basics of Artificial Neural Networks <a></a>\n\n\n![ANN](https://upload.wikimedia.org/wikipedia/commons/4/46/Colored_neural_network.svg)\n\n### Single-layer and Multi-layer perceptrons\n\n多层感知器(Multi-Layer Perceptron，MLP)也叫人工神经网络(Artificial Neural Network，ANN)，除了输入输出层，它中间可以有多个隐层。最简单的MLP需要有一层隐层，即输入层、隐层和输出层才能称为一个简单的神经网络。习惯原因我之后会称为神经网络。通俗而言，神经网络是仿生物神经网络而来的一种技术，通过连接多个特征值，经过线性和非线性的组合，最终达到一个目标，这个目标可以是识别这个图片是不是一只猫，是不是一条狗或者属于哪个分布。\n\n单层感知器(SLP)是基于阈值传递函数的前馈网络。SLP是最简单的人工神经网络类型，只能用二元目标(1,0)对线性可分的情况进行分类。\n\n因为SLP是一个线性分类器如果情况不是线性可分的学习过程永远不会达到所有情况都被正确分类的点。感知器无法解决线性不可分问题的最著名的例子就是异或问题。\n\n\n多层感知器(MLP)具有与单层感知器相同的结构，具有一个或多个隐藏层。反向传播算法包括两个阶段:激活的前进阶段传播从输入到输出层之间的误差和落后的阶段,观察到的实际和请求的标称值在输出层向后传播以修改重量和偏差值。\n\n由此可知，神经网络主要有三个基本要素：权重、偏置和激活函数\n\n权重：神经元之间的连接强度由权重表示，权重的大小表示可能性的大小\n\n偏置：偏置的设置是为了正确分类样本，是模型中一个重要的参数，即保证通过输入算出的输出值不能随便激活。\n\n激活函数：起非线性映射的作用，其可将神经元的输出幅度限制在一定范围内，一般限制在（-1~1）或（0~1）之间。最常用的激活函数是Sigmoid函数，其可将（-∞，+∞）的数映射到（0~1）的范围内。\n","metadata":{}},{"cell_type":"markdown","source":"# 关于本数据集\n\n   iris包含150个样本，对应数据集的每行数据。每行数据包含每个样本的四个特征和样本的类别信息，所以iris数据集是一个150行5列的二维表。iris包含150个样本，对应数据集的每行数据。每行数据包含每个样本的四个特征和样本的类别信息，所以iris数据集是一个150行5列的二维表。\n\n\n![dattaset](http://datahref.com/wp-content/uploads/2016/06/Iris_dataset_scatterplot.svg_.png)\n","metadata":{}},{"cell_type":"markdown","source":"# Perceptron\n\n\n感知器是模拟人类神经元的基本功能。它接收𝑛输入，与树突输入的神经元相关联。由于倾斜，每个树突都被一个数字加权，这个数字表示它与神经元[1]的输入相关性。\n\nThe perceptron is a basic function that mimics the human neuron. It receives $n$ inputs, associated to the dendrites inputs to the neuron. Each dendrite, due to *lernging*, is weighted by a number that signals its input relevance for the neuron [1]. \n\n![Neuron](https://upload.wikimedia.org/wikipedia/commons/a/a9/Complete_neuron_cell_diagram_en.svg)\n\n\n信号就这样被制造出来，并通过*轴突*传递给其他神经元[2];实际上，只有当精心设计的输入超过某个阈值时，神经元才会发出信号;这是一个尖峰神经元\nThe signal is thus elaborated and passed through the *axon* to others neurons [2]; actually, the neurons *fires* the signal only if the elaborated inputs have surpassed a certain threshold; this is a spiking neuron [3].\n\n\n感知器想要模仿它。接收一个向量(即数组)$x_i$信号，其中$i$代表$i$-th树突，它用一个$w_i$向量对每个树突加权。它还增加了一个“偏差”来消除接近零的问题(偏差将决策边界从原点移开，并且不依赖于任何输入值)。\n\nThe perceptron wants to mimic it. Receinving a vector (i.e. array) $x_i$ of signals, where $i$ stands for the $i$-th dendrites, it weights each of them by a vector of weights $w_i$. It adds also a *bias* to remove near-zero issues (the bias shifts the decision boundary away from the origin and does not depend on any input value).\n\n![Perceptron](https://miro.medium.com/max/2870/1*n6sJ4yZQzwKL9wnF5wnVNg.png)\n\n### Activation functions\n\n激活函数（activation function）通过计算加权和并加上偏置来确定神经元是否应该被激活， 它们将输入信号转换为输出的可微运算。 大多数激活函数都是非线性的。\n\n####  ReLU函数\n最受欢迎的激活函数是修正线性单元（Rectified linear unit，ReLU）， 因为它实现简单，同时在各种预测任务中表现良好。 ReLU提供了一种非常简单的非线性变换。 给定元素，ReLU函数被定义为该元素与的最大值：\n$$\\operatorname{ReLU}(x) = \\max(x, 0).$$\n\n\n通俗地说，ReLU函数通过将相应的活性值设为0，仅保留正元素并丢弃所有负元素。 为了直观感受一下，我们可以画出函数的曲线图。 正如从图中所看到，激活函数是分段线性的。\n\n![5555](http://zh.d2l.ai/_images/output_mlp_76f463_21_0.svg)\n\n\n当输入为负时，ReLU函数的导数为0，而当输入为正时，ReLU函数的导数为1。 注意，当输入值精确等于0时，ReLU函数不可导。 在此时，我们默认使用左侧的导数，即当输入为0时导数为0。 我们可以忽略这种情况，因为输入可能永远都不会是0。 这里引用一句古老的谚语，“如果微妙的边界条件很重要，我们很可能是在研究数学而非工程”， 这个观点正好适用于这里。 下面我们绘制ReLU函数的导数。\n\n\n![666](http://zh.d2l.ai/_images/output_mlp_76f463_33_0.svg)\n\n使用ReLU的原因是，它求导表现得特别好：要么让参数消失，要么让参数通过。 这使得优化表现得更好，并且ReLU减轻了困扰以往神经网络的梯度消失问题。\n\n#### sigmoid函数\n\n对于一个定义域在中的输入， sigmoid函数将输入变换为区间(0, 1)上的输出。 因此，sigmoid通常称为挤压函数（squashing function）： 它将范围（-inf, inf）中的任意输入压缩到区间（0, 1）中的某个值：\n\n$$f (x) = \\frac{1}{1+e^{-x}} \\,,$$\n\n在最早的神经网络中，科学家们感兴趣的是对“激发”或“不激发”的生物神经元进行建模。 因此，这一领域的先驱可以一直追溯到人工神经元的发明者麦卡洛克和皮茨，他们专注于阈值单元。 阈值单元在其输入低于某个阈值时取值0，当输入超过阈值时取值1。\n\n当人们逐渐关注到到基于梯度的学习时， sigmoid函数是一个自然的选择，因为它是一个平滑的、可微的阈值单元近似。 当我们想要将输出视作二元分类问题的概率时， sigmoid仍然被广泛用作输出单元上的激活函数 （你可以将sigmoid视为softmax的特例）。 然而，sigmoid在隐藏层中已经较少使用， 它在大部分时候被更简单、更容易训练的ReLU所取代。 在后面关于循环神经网络的章节中，我们将描述利用sigmoid单元来控制时序信息流的架构。\n\n\n当输入接近0时，sigmoid函数接近线性变换。\n\n\n![sigmoid](https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg)\n\nsigmoid函数的导数为下面的公式：\n\n\n$$\\frac{d}{dx} \\operatorname{sigmoid}(x) = \\frac{\\exp(-x)}{(1 + \\exp(-x))^2} = \\operatorname{sigmoid}(x)\\left(1-\\operatorname{sigmoid}(x)\\right).$$\n\n\n![sigmoid2](http://zh.d2l.ai/_images/output_mlp_76f463_57_0.svg)\n\n\n#### tanh函数\n\n与sigmoid函数类似， tanh(双曲正切)函数也能将其输入压缩转换到区间(-1, 1)上。 tanh函数的公式如下：\n$$\\operatorname{tanh}(x) = \\frac{1 - \\exp(-2x)}{1 + \\exp(-2x)}.$$\n下面我们绘制tanh函数。 注意，当输入在0附近时，tanh函数接近线性变换。 函数的形状类似于sigmoid函数， 不同的是tanh函数关于坐标系原点中心对称。\n\n![sigmoid2](https://zh.d2l.ai/_images/output_mlp_76f463_69_0.svg)\n\ntanh函数的导数是：\n$$\\frac{d}{dx} \\operatorname{tanh}(x) = 1 - \\operatorname{tanh}^2(x).$$\n\ntanh函数的导数图像如下所示。 当输入接近0时，tanh函数的导数接近最大值1。 与我们在sigmoid函数图像中看到的类似， 输入在任一方向上越远离0点，导数越接近0。\n\n![sigmoid2](https://zh.d2l.ai/_images/output_mlp_76f463_81_0.svg#pic_center)\n\n","metadata":{}},{"cell_type":"code","source":"# Define the sigmoid activator; we ask if we want the sigmoid or its derivative\ndef sigmoid_act(x, der=False):\n    import numpy as np\n    \n    if (der==True) : #derivative of the sigmoid\n        f = x/(1-x)\n    else : # sigmoid\n        f = 1/(1+ np.exp(-x))\n    \n    return f\n\n# We may employ the Rectifier Linear Unit (ReLU)\ndef ReLU_act(x, der=False):\n    import numpy as np\n    \n    if (der== True):\n        if x>0 :\n            f= 1\n        else :\n            f = 0\n    else :\n        if x>0:\n            f = x\n        else :\n            f = 0\n    return f\n\n# Now we are ready to define the perceptron; \n# it eats a np.array (that may be a list of features )\ndef perceptron(X, act='Sigmoid'): \n    import numpy as np\n    \n    shapes = X.shape # Pick the number of (rows, columns)!\n    n= shapes[0]+shapes[1]\n    # Generating random weights and bias\n    w = 2*np.random.random(shapes) - 0.5 # We want w to be between -1 and 1\n    b = np.random.random(1)\n    \n    # Initialize the function\n    f = b[0]\n    for i in range(0, X.shape[0]-1) : # run over column elements\n        for j in range(0, X.shape[1]-1) : # run over rows elements\n            f += w[i, j]*X[i,j]/n\n    # Pass it to the activation function and return it as an output\n    if act == 'Sigmoid':\n        output = sigmoid_act(f)\n    else :\n        output = ReLU_act(f)\n        \n    return output\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Neural Network's Layer(s)\n\nA standard Artificial Neural Network will be made of multiple layers:\n1. An **Input Layer**, that pass the features to the NN\n2. An arbitrary number of **Hidden Layers**, containing an arbitrary number of neurons for each layer, that receives the inputs and elaborate them. We will introduce Hidden Layers with ReLU activator, since in the *hidden* part of the NN we don't need the output to be contained in the $[0,1]$ range. \n3. An **Output Layer**: these layers contains a number of neurons equal to the number of possible labels we want to have a prediction to; this is because the output of the NN is thus a vector whose dimension is the same as the cardinality of the set of labels, and its entries are the *probability* for each label for the element whose feateures we have passed to the NN. This means that we will use a sigmoid activator to the Output layer, so we squeeze each perceptron's output between 0 and 1. \n\n**输出层**:这些层包含的神经元数量等于我们想要预测的可能标签的数量;这是因为神经网络的输出是一个向量，它的维数与标签集的基数相同，它的值是我们已经传递给神经网络的元素的每个标签的*概率*。在Output层使用sigmoid激活器，我们将每个感知器的输出压缩到0和1之间。\n\n\n![ANN](https://miro.medium.com/proxy/1*DW0Ccmj1hZ0OvSXi7Kz5MQ.jpeg#pic_center)\n\n在本实验中，我们有3种花的分类;输出层有三个节点，选择每行最大的值作为该类的标签，赋值为1，其余的赋值为0\n\n\n## Backpropagation and Gradien Descent\n\nBP网络的结构降法的基础上。BP网络的输入输出关系实质上是一种映射关系：一个 输入m输出的BP神经网络所完成的功能是从 一维欧氏空间向m维欧氏空间中一有限域的连续映射，这一映射具有高度非线性。它的信息处理能力来源于简单非线性函数的多次复合，因此具有很强的函数复现能力。这是BP算法得以应用的基础。\n\n反向传播算法主要由两个环节(激励传播、权重更新)反复循环迭代，直到网络的对输入的响应达到预定的目标范围为止。\n\nBP算法的学习过程由正向传播过程和反向传播过程组成。在正向传播过程中，输入信息通过输入层经隐含层，逐层处理并传向输出层。如果在输出层得不到期望的输出值，则取输出与期望的误差的平方和作为目标函数，转入反向传播，逐层求出目标函数对各神经元权值的偏导数，构成目标函数对权值向量的梯量，作为修改权值的依据，网络的学习在权值修改过程中完成。误差达到所期望值时，网络学习结束。\n\n\n通过计算梯度使代价函数最小化，得到局部极小值。在每个迭代或训练步骤中，网络中的权重通过计算的梯度以及 learning rate 进行更新，该梯度控制对权重值的修改因子。在神经网络的训练阶段，每个步骤都要重复这个过程。理想情况下，目标是在每一步之后更接近局部最小值。\n\n\n\n\n\n\n\n为了调整训练参数 $\\{w\\}$ and $\\{b\\}$, 我们需要实现“反向传播”. 我们要最小化某个**成本函数**\n$$\\mu(y,\\bar{y})=|y-\\bar{y}|^2$$\n$y$是输出层的输出，而$\\bar{y}$是实际的标签; 为了做到这一点，我们开始*梯度下降*, 这意味着我们把成本函数看作是可训练参数的函数 $\\mathbf{w}$ 像 $\\{w\\}$ 和 $\\{b\\}$, 我们计算梯度 - 梯度，可以看作是多维图的斜率 [3] - 从随机初始化的集合中减去它\n$$\\mathbf{w}'_n =  \\mathbf{w}_n - \\eta \\nabla \\mu(\\mathbf{w}_n)  \\,,$$\n向代价函数的最优值，或全局最小值移动。 上式中 $\\eta$ 是神经网络的学习率 [4] \n\n\n如下图所示 [5]:\n\n![GD](https://hackernoon.com/hn-images/1*f9a162GhpMbiTVTAua_lLQ.png#pic_center)\n\n\n\n\n\n\n### 回顾\n\nWe now recap what we have shown: \n* **Input**: 设置输入 $z_0^{i_0}$;\n* **Feed Forward**: 计算$\\alpha$-th 层 $z_\\alpha^{i_\\alpha}$ \n$$z_\\alpha^{i_\\alpha} = \\varphi ( w_{\\alpha}^{i_{\\alpha} i_{\\alpha-1}} \\, z^{i_{\\alpha-1}}_{\\alpha-1} + b_\\alpha^{i_\\alpha} ) \\equiv \\varphi (\\zeta^{i_\\alpha}_\\alpha) ,$$\n直到输出层, 利用的公式为\n$$y = f( w_{Out}^{i_n} z_n^{i_n} + b_{Out} ) .$$\n* **计算偏差**: 通过公式计算最后一层的误差\n$$\\delta^N_{i_N} = \\frac{\\partial \\mu}{\\partial z_N^{i_N} } \\,.$$\n最后一层实际上是输出层，所以\n$$\\delta^{Out} = \\frac{\\partial \\mu}{\\partial z_{Out} } \\cdot f' (z_{Out}) .$$\n*  **Backpropagate the Error**: 对每一层 $\\alpha= N-1, \\ldots, 2$ 计算 \n$$\\delta_{\\alpha}^{i_\\alpha}  = {\\large \\Sigma}_{{i_\\alpha}}  \\delta_{\\alpha+1}^{i_\\alpha+1}  \\, w_{\\alpha}^{i_{\\alpha+1} i_\\alpha} \\, \\varphi' (z_{\\alpha}^{i_\\alpha}) \\,.$$\n* **Output**: 损失函数的梯度计算\n\n$$\\frac{\\partial \\mu}{\\partial w_\\alpha^{i_{\\alpha+1} i_\\alpha} } = \\delta_\\alpha^{i_\\alpha} \\cdot z_{\\alpha-1}^{i_\\alpha-1} \\varphi'(\\zeta_\\alpha^{i_\\alpha} ) ,$$\n$$\\frac{\\partial \\mu}{\\partial b_\\alpha^{i_{\\alpha}} } =  \\delta_\\alpha^{i_\\alpha} \\cdot  \\varphi'(\\zeta_\\alpha^{i_\\alpha} ) . $$\n\n激活函数的显式导数是已知的，很容易计算，因为\n$$f'(x) = f(x)(1-f(x)) = y(1-y) \\,,$$\n\n\n\n\n\n\n## 三层网络\n \n将初始化一个人工神经网络\n\n1. An **输入层** 输入的是4列dataframe  4个神经元\n\n2. **隐藏层** 4个神经元\n\n3. 因为我们要对三种花色进行分类，所以输出层为3个节点（神经元）\n\n\n\n我们将会进行如下操作\n\n0. 对数据集进行 train/test split  接下来分别用批量梯度下降和随机梯度下降来确定batch值\n\n1. 输入训练数据$\\{x_i\\}_I$，并将其送入第一层;\n2. 对每行数据\n    1. **前馈**: \n    $$z_1^j = \\varphi( {\\large \\Sigma}_{i=0,1} \\, w_1^{j i} x^i + b_1^j ), $$\n    $$z_2^k = \\varphi( {\\large \\Sigma}_{j=0,\\ldots, p-1} \\, w_2^{k j} z_1^j + b_2^k ), $$\n    $$y = f( {\\large \\Sigma}_{k=0, \\ldots, q-1} w_{Out}^k z_2^k + b_{Out} ) .$$\n    2.  **误差计算**: \n    $$\\delta_{Out} = \\frac{\\partial \\mu}{\\partial y} \\cdot f' (y) = 2(y-\\bar{y}) \\cdot y(1-y) $$\n    3. **反向传播**: \n    $$\\delta_2^{k} =   \\delta_{Out}  \\, w_{Out}^{k} \\, \\varphi' (z_{2}^{k}) ,$$\n    $$\\delta_1^j = {\\large \\Sigma}_{k=0, \\ldots, q-1} \\delta_2^{k} w_2^{k j}  \\, \\varphi' (z_{1}^{j}) . $$\n    \n    \n3. **梯度下降**: 当我们从训练集中的$I$传递到$(I+1)$时，我们需要按照规则顺序更新权重*\n    * 输出层:\n    $${}^{(I+1)}w_{Out}^k = {}^{(I)} w_{Out}^k - \\eta {}^{(I)}\\delta_{Out} \\, z_2^k \\,,$$\n    $${}^{(I+1)}b_{Out} = {}^{(I)} b_{Out} - \\eta {}^{(I)}\\delta_{Out}\\,,$$\n    * 1层:\n    $${}^{(I+1)}w_{1}^{ji} = {}^{(I)} w_{1}^{ji} - \\eta \\delta_1^{j} \\, x_i^{i} \\,,$$\n    $${}^{(I+1)}b_{1} = {}^{(I)} b_{1}^j - \\eta \\delta_2^{j} \\,, $$\n\n遍历整个集合，这样我们就可以“训练”我们的神经网络!\n","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras import Input\nfrom keras.layers import Dense\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nprint('Tensorflow/Keras: %s' % keras.__version__) # print version\n\n# Sklearn\nimport sklearn # for model evaluation\nprint('sklearn: %s' % sklearn.__version__) # print version\nfrom sklearn.model_selection import train_test_split # for splitting data into train and test samples\nfrom sklearn.metrics import classification_report # for model evaluation metrics\n\n# Visualization\nimport plotly \nimport plotly.express as px\nimport plotly.graph_objects as go\nprint('plotly: %s' % plotly.__version__) # print version\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-20T07:55:17.536257Z","iopub.execute_input":"2022-04-20T07:55:17.536551Z","iopub.status.idle":"2022-04-20T07:55:17.549568Z","shell.execute_reply.started":"2022-04-20T07:55:17.53652Z","shell.execute_reply":"2022-04-20T07:55:17.548859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"加在数据集","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/iris/Iris.csv')\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-20T07:55:17.662875Z","iopub.execute_input":"2022-04-20T07:55:17.663305Z","iopub.status.idle":"2022-04-20T07:55:17.680085Z","shell.execute_reply.started":"2022-04-20T07:55:17.663273Z","shell.execute_reply":"2022-04-20T07:55:17.679124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"对数据集进行修改，增加3个标志位 ***flag1***，***flag2***，***flag3***，品种为Iris-setosa的花flag1位值为1，若某花不属于Iris-setosa品种，flag1位置为0，flag2，flag3同理。","metadata":{}},{"cell_type":"code","source":"df[\"flag1\"] = df[\"Species\"].apply(lambda x: 1 if x==\"Iris-setosa\" else 0)\ndf[\"flag2\"] = df[\"Species\"].apply(lambda x: 1 if x==\"Iris-versicolor\" else 0)\ndf[\"flag3\"] = df[\"Species\"].apply(lambda x: 1 if x==\"Iris-virginica\" else 0)\n\n\n\n# 试试这种方法\n# # We define a dictionary to binarize the sex\n# dict_sex = {\n#     'male' : 0,\n#     'female' : 1\n# }\n\n# # We apply the dictionary using a lambda function and the pandas .apply() module\n# data['Bsex'] = data['Sex'].apply(lambda x : dict_sex[x])\n\n\n\n\n\ndf","metadata":{"execution":{"iopub.status.busy":"2022-04-20T07:55:17.855983Z","iopub.execute_input":"2022-04-20T07:55:17.856579Z","iopub.status.idle":"2022-04-20T07:55:17.879606Z","shell.execute_reply.started":"2022-04-20T07:55:17.856539Z","shell.execute_reply":"2022-04-20T07:55:17.87897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=df.iloc[:,1:5]\ny=df.iloc[:,6:9]\n# y=df['flag'].values\nX,y","metadata":{"execution":{"iopub.status.busy":"2022-04-20T07:55:18.11139Z","iopub.execute_input":"2022-04-20T07:55:18.112318Z","iopub.status.idle":"2022-04-20T07:55:18.127761Z","shell.execute_reply.started":"2022-04-20T07:55:18.112259Z","shell.execute_reply":"2022-04-20T07:55:18.126717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T07:55:18.333401Z","iopub.execute_input":"2022-04-20T07:55:18.333729Z","iopub.status.idle":"2022-04-20T07:55:18.341244Z","shell.execute_reply.started":"2022-04-20T07:55:18.333695Z","shell.execute_reply":"2022-04-20T07:55:18.34023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specify the structure of a Neural Network\nmodel = Sequential(name=\"Model_Input\") # Model\nmodel.add(Input(shape=(4,), name='Input-Layer')) # Input Layer - need to speicfy the shape of inputs\nmodel.add(Dense(4, activation='softplus', name='Hidden-Layer')) # Hidden Layer, softplus(x) = log(exp(x) + 1)\nmodel.add(Dense(3, activation='sigmoid', name='Output-Layer')) # Output Layer, sigmoid(x) = 1 / (1 + exp(-x))\n","metadata":{"execution":{"iopub.status.busy":"2022-04-20T07:55:18.448869Z","iopub.execute_input":"2022-04-20T07:55:18.449617Z","iopub.status.idle":"2022-04-20T07:55:18.474724Z","shell.execute_reply.started":"2022-04-20T07:55:18.449583Z","shell.execute_reply":"2022-04-20T07:55:18.474088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compile keras model\nmodel.compile(optimizer='adam', # default='rmsprop', an algorithm to be used in backpropagation\n              loss='binary_crossentropy', # Loss function to be optimized. A string (name of loss function), or a tf.keras.losses.Loss instance.\n              metrics=['Accuracy', 'Precision', 'Recall'], # List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a tf.keras.metrics.Metric instance. \n              loss_weights=None, # default=None, Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs.\n              weighted_metrics=None, # default=None, List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing.\n              run_eagerly=None, # Defaults to False. If True, this Model's logic will not be wrapped in a tf.function. Recommended to leave this as None unless your Model cannot be run inside a tf.function.\n              steps_per_execution=None # Defaults to 1. The number of batches to run during each tf.function call. Running multiple batches inside a single tf.function call can greatly improve performance on TPUs or small models with a large Python overhead.\n             )\n\n# or loss = Sigmoid Cross-Entropy","metadata":{"execution":{"iopub.status.busy":"2022-04-20T07:55:18.635017Z","iopub.execute_input":"2022-04-20T07:55:18.635944Z","iopub.status.idle":"2022-04-20T07:55:18.645116Z","shell.execute_reply.started":"2022-04-20T07:55:18.635903Z","shell.execute_reply":"2022-04-20T07:55:18.644441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,y_train","metadata":{"execution":{"iopub.status.busy":"2022-04-20T07:55:18.826585Z","iopub.execute_input":"2022-04-20T07:55:18.827291Z","iopub.status.idle":"2022-04-20T07:55:18.840231Z","shell.execute_reply.started":"2022-04-20T07:55:18.827246Z","shell.execute_reply":"2022-04-20T07:55:18.839212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##### Step 5 - Fit keras model on the dataset\nmodel.fit(X_train, # input data\n          y_train, # target data\n          batch_size=120, # Number of samples per gradient update. If unspecified, batch_size will default to 32.\n          epochs=500, # default=1, Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided\n          verbose='auto', # default='auto', ('auto', 0, 1, or 2). Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with ParameterServerStrategy.\n          callbacks=None, # default=None, list of callbacks to apply during training. See tf.keras.callbacks\n          validation_split=0.2, # default=0.0, Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. \n          #validation_data=(X_test, y_test), # default=None, Data on which to evaluate the loss and any model metrics at the end of each epoch. \n          shuffle=True, # default=True, Boolean (whether to shuffle the training data before each epoch) or str (for 'batch').\n          class_weight=None, # default=None, Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class.\n          sample_weight=None, # default=None, Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only).\n          initial_epoch=0, # Integer, default=0, Epoch at which to start training (useful for resuming a previous training run).\n          steps_per_epoch=None, # Integer or None, default=None, Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. \n          validation_steps=None, # Only relevant if validation_data is provided and is a tf.data dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch.\n          validation_batch_size=None, # Integer or None, default=None, Number of samples per validation batch. If unspecified, will default to batch_size.\n          validation_freq=3, # default=1, Only relevant if validation data is provided. If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs.\n          max_queue_size=10, # default=10, Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10.\n          workers=1, # default=1, Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1.\n          use_multiprocessing=False, # default=False, Used for generator or keras.utils.Sequence input only. If True, use process-based threading. If unspecified, use_multiprocessing will default to False. \n         )","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"asss = model.predict(X_train)\ndf_result = pd.DataFrame(asss)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T07:55:29.235312Z","iopub.execute_input":"2022-04-20T07:55:29.235622Z","iopub.status.idle":"2022-04-20T07:55:29.341057Z","shell.execute_reply.started":"2022-04-20T07:55:29.235583Z","shell.execute_reply":"2022-04-20T07:55:29.340192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_result","metadata":{"execution":{"iopub.status.busy":"2022-04-20T07:55:29.342304Z","iopub.execute_input":"2022-04-20T07:55:29.34253Z","iopub.status.idle":"2022-04-20T07:55:29.354224Z","shell.execute_reply.started":"2022-04-20T07:55:29.342502Z","shell.execute_reply":"2022-04-20T07:55:29.353689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for x in range(120):\n    df_result.loc[x,df_result[x:x+1].idxmax(axis=\"columns\")]=1\n    \ndf_result = (df_result == 1.0).astype(int)\ndf_result,y_train","metadata":{"execution":{"iopub.status.busy":"2022-04-20T07:55:29.356423Z","iopub.execute_input":"2022-04-20T07:55:29.356989Z","iopub.status.idle":"2022-04-20T07:55:29.548446Z","shell.execute_reply.started":"2022-04-20T07:55:29.356947Z","shell.execute_reply":"2022-04-20T07:55:29.547492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_train, df_result))","metadata":{"execution":{"iopub.status.busy":"2022-04-20T07:55:29.55024Z","iopub.execute_input":"2022-04-20T07:55:29.550535Z","iopub.status.idle":"2022-04-20T07:55:29.564759Z","shell.execute_reply.started":"2022-04-20T07:55:29.550495Z","shell.execute_reply":"2022-04-20T07:55:29.564136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary() ","metadata":{"execution":{"iopub.status.busy":"2022-04-20T07:55:29.565832Z","iopub.execute_input":"2022-04-20T07:55:29.566156Z","iopub.status.idle":"2022-04-20T07:55:29.574256Z","shell.execute_reply.started":"2022-04-20T07:55:29.566129Z","shell.execute_reply":"2022-04-20T07:55:29.572931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"attt = model.predict(X_test)\nkkkk = pd.DataFrame(attt)\n\nfor y in range(30):\n    kkkk.loc[y,kkkk[y:y+1].idxmax(axis=\"columns\")]=1\n    \nkkkk = (df_result2 == 1).astype(int)\nkkkk,y_test\n\nprint(classification_report(y_test, df_result2))","metadata":{"execution":{"iopub.status.busy":"2022-04-20T07:55:29.575571Z","iopub.execute_input":"2022-04-20T07:55:29.575998Z","iopub.status.idle":"2022-04-20T07:55:29.685831Z","shell.execute_reply.started":"2022-04-20T07:55:29.575967Z","shell.execute_reply":"2022-04-20T07:55:29.685234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_result.to_csv('df_result.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T07:55:29.686886Z","iopub.execute_input":"2022-04-20T07:55:29.687185Z","iopub.status.idle":"2022-04-20T07:55:29.691508Z","shell.execute_reply.started":"2022-04-20T07:55:29.687158Z","shell.execute_reply":"2022-04-20T07:55:29.690815Z"},"trusted":true},"execution_count":null,"outputs":[]}]}